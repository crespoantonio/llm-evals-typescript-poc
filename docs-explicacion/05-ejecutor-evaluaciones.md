# ğŸ­ Ejecutor de Evaluaciones - El Director de Orquesta

## Â¿QuÃ© es el Ejecutor de Evaluaciones?

El **EvalRunner** es como el **director de una orquesta sinfÃ³nica**. No toca ningÃºn instrumento, pero **coordina a todos los mÃºsicos** para crear una sinfonÃ­a perfecta.

En este caso, los "mÃºsicos" son:
- ğŸ¤– **Clientes LLM** â†’ Los que hablan con la IA
- ğŸ“Š **Cargador de datos** â†’ El que trae las preguntas
- ğŸ§ª **Templates de evaluaciÃ³n** â†’ Los que califican las respuestas
- ğŸ’¾ **Sistema de cachÃ©** â†’ El que ahorra dinero
- ğŸ“ˆ **MÃ©tricas personalizadas** â†’ Los que miden la calidad
- ğŸ—„ï¸ **Base de datos** â†’ El que guarda todo

## ğŸ—ï¸ Arquitectura del EvalRunner

### Los Componentes Internos
```typescript
export class EvalRunner {
  private registry: Registry;           // ğŸ“‹ CatÃ¡logo de evaluaciones
  private logger: Logger;               // ğŸ“ Sistema de logs  
  private costManager: CostManager;     // ğŸ’° Calculadora de costos
  private cache: EvaluationCache;       // âš¡ Sistema de cachÃ©
  private metricsRegistry: MetricsRegistry; // ğŸ“Š Registro de mÃ©tricas
  private store: EvaluationStore;       // ğŸ—„ï¸ Base de datos
}
```

**Es como un director que tiene:**
- **ğŸ“‹ La partitura** (registry) â†’ Sabe quÃ© mÃºsica tocar
- **ğŸ“ El cuaderno de notas** (logger) â†’ Registra todo lo que pasa
- **ğŸ’° El contador** (costManager) â†’ Lleva la cuenta de los gastos
- **âš¡ La memoria** (cache) â†’ Recuerda lo que ya se hizo
- **ğŸ“Š Los crÃ­ticos** (metricsRegistry) â†’ EvalÃºan la calidad
- **ğŸ—„ï¸ El archivo** (store) â†’ Guarda las grabaciones

## ğŸµ La SinfonÃ­a de una EvaluaciÃ³n

### Acto 1: ğŸ“‹ PreparaciÃ³n (Setup)
```typescript
async runEval(options: RunOptions): Promise<EvalReport>
```

**Â¿QuÃ© pasa en la preparaciÃ³n?**

1. **ğŸ¬ Generar ID Ãºnico de ejecuciÃ³n**
```typescript
const runId = this.generateRunId();
// Ejemplo: "20241231143045AB12CD"
```

2. **ğŸ“‹ Cargar configuraciÃ³n de la evaluaciÃ³n**
```typescript
const config = this.registry.getConfig(options.eval);
// Obtiene: tipo de evaluaciÃ³n, parÃ¡metros, datasets, etc.
```

3. **ğŸ“Š Cargar dataset de preguntas**
```typescript
const dataset = await loadDataset(datasetPath);
const samples = options.max_samples 
  ? dataset.samples.slice(0, options.max_samples)  // Limitar si es necesario
  : dataset.samples;
```

4. **ğŸ¤– Crear cliente LLM**
```typescript
const llmClient = createLLMClient(options.model, options.timeout);
// Ejemplo: Cliente para GPT-4 con timeout de 60 segundos
```

### Acto 2: ğŸ§ª EjecuciÃ³n (El Bucle Principal)

**Para cada pregunta del dataset:**

```typescript
for (let i = 0; i < samples.length; i++) {
  const sample = samples[i];
  
  // ğŸ’¾ 1. Verificar cachÃ© primero (ahorrar dinero)
  let completion = await this.cache.getCachedResult(model, sample, options);
  
  if (completion) {
    console.log('ğŸ’¾ Cache hit! - No pagamos por esta pregunta');
  } else {
    // ğŸ’¸ 2. Preguntar al modelo (esto cuesta dinero)
    completion = await llmClient.complete(sample.input, completionOptions);
    
    // ğŸ’¾ 3. Guardar en cachÃ© para la prÃ³xima vez
    await this.cache.setCachedResult(model, sample, options, completion);
  }
  
  // ğŸ§ª 4. Evaluar la respuesta
  const result = await template.evaluate(sample, completion);
  results.push(result);
  
  // ğŸ“ 5. Registrar todo lo que pasÃ³
  await this.logger.logEvent({
    type: 'sampling',
    data: { input: sample.input, completion: completion.content }
  });
}
```

### Acto 3: ğŸ“Š AnÃ¡lisis Final

```typescript
// ğŸ“Š Calcular mÃ©tricas bÃ¡sicas
const totalSamples = results.length;
const correct = results.filter(r => r.passed).length;
const score = correct / totalSamples;

// ğŸ’° Calcular costos
const tokenUsage = this.calculateTokenUsage(options.model, results);

// ğŸ“ˆ Calcular mÃ©tricas personalizadas
const customMetrics = await this.metricsRegistry.calculateAllMetrics(results, report);
```

## ğŸ¯ Funciones Especiales del EvalRunner

### 1. ğŸƒâ€â™‚ï¸ Modo Dry Run (SimulaciÃ³n)

```typescript
if (options.dry_run) {
  console.log('[DRY RUN] Sample 1:');
  console.log('  Input: "Â¿CuÃ¡nto es 2+2?"');
  console.log('  Expected: "4"');
  // âœ… No hace peticiones reales a la API
  // âœ… No gasta dinero  
  // âœ… Te muestra quÃ© va a pasar
}
```

**Â¿CuÃ¡ndo usar dry run?**
- **ğŸ” Verificar que el dataset estÃ¡ bien** antes de gastar dinero
- **âš™ï¸ Probar configuraciones** nuevas sin riesgo
- **ğŸ“Š Estimar costos** antes de ejecutar

### 2. âš¡ Sistema de CachÃ© Inteligente

```typescript
// ğŸ”‘ Clave de cachÃ©: modelo + pregunta + configuraciÃ³n
const cacheKey = {
  model: 'gpt-4',
  input: [{"role": "user", "content": "Â¿CuÃ¡nto es 2+2?"}],
  options: { temperature: 0.0, max_tokens: 10 }
};

// ğŸ’¾ Si ya hicimos esta pregunta exacta antes...
if (cache.has(cacheKey)) {
  return cache.get(cacheKey);  // âœ… Â¡Gratis!
} else {
  const result = await llm.complete(...);  // ğŸ’¸ Pagamos
  cache.set(cacheKey, result);  // ğŸ’¾ Guardamos para la prÃ³xima
  return result;
}
```

**Beneficios del cachÃ©:**
- **ğŸ’° Ahorro de dinero** â†’ No pagas dos veces por la misma pregunta
- **âš¡ Velocidad** â†’ Respuesta instantÃ¡nea para preguntas repetidas
- **ğŸ§ª Reproducibilidad** â†’ Mismos resultados en mÃºltiples ejecuciones

### 3. ğŸ“Š Indicadores de Progreso Inteligentes

```typescript
// Modo normal (limpio)
Progress: 15/100

// Modo verbose (detallado)  
Progress: 15/100 (15%)
âœ… Sample 15 - Score: 0.85 - PASS
   Reasoning: La respuesta '4' coincide exactamente con el ideal '4'
ğŸ’¾ Cache hit for sample 16
```

### 4. ğŸš¨ Manejo Robusto de Errores

```typescript
try {
  const completion = await llmClient.complete(sample.input);
  const result = await template.evaluate(sample, completion);
} catch (error) {
  console.error(`âŒ Error evaluating sample ${i + 1}: API timeout`);
  
  // âœ… No se rompe todo, crea un resultado de error
  const failedResult = {
    sample_id: `sample_${i}`,
    score: 0,
    passed: false,
    reasoning: 'Evaluation error: API timeout',
    metadata: { error: true }
  };
  results.push(failedResult);
}
```

## ğŸ’° Calculadora de Costos Integrada

```typescript
private calculateTokenUsage(model: string, results: EvalResult[]): TokenUsage {
  // ğŸ“Š Recopilar estadÃ­sticas
  const totalPromptTokens = results.reduce(/* suma todos los tokens de prompt */);
  const totalCompletionTokens = results.reduce(/* suma todos los tokens de respuesta */);
  
  // ğŸ’° Calcular costos usando el CostManager
  const totalCost = results.reduce((sum, result) => {
    return sum + this.costManager.calculateCompletionCost(model, result.completion);
  }, 0);
  
  // ğŸ“ˆ Calcular estadÃ­sticas Ãºtiles
  return {
    total_tokens: totalPromptTokens + totalCompletionTokens,
    average_tokens_per_sample: Math.round(totalTokens / results.length),
    estimated_cost: totalCost,
    cost_breakdown: {
      prompt_cost: promptCost,        // Costo de las preguntas
      completion_cost: completionCost // Costo de las respuestas
    }
  };
}
```

## ğŸ“Š Reporte Final Detallado

```text
==================================================
ğŸ¯ Final Results:
   Total samples: 100
   Correct: 87
   Incorrect: 13
   Accuracy: 87.00%
   Duration: 45.32s

ğŸ“Š Token Usage:
   â€¢ Prompt tokens: 12,450
   â€¢ Completion tokens: 3,280
   â€¢ Total tokens: 15,730
   â€¢ Avg tokens/sample: 157

ğŸ’° Estimated Cost:
   â€¢ Total: $0.2359
   â€¢ Prompt cost: $0.1245
   â€¢ Completion cost: $0.1114
   â€¢ Cost per sample: $0.0024

ğŸ“ˆ Custom Metrics:
   ğŸ¯ ACCURACY:
      â†—ï¸ Response Consistency: 94.2%
         Measures how consistent responses are across similar inputs
   âš¡ EFFICIENCY:  
      â†™ï¸ Token Efficiency: 157 tokens/sample
         Lower is better - measures conciseness of responses
   ğŸ’° COST:
      â†™ï¸ Cost Efficiency: $0.0024/sample
         Lower is better - cost per evaluation sample

ğŸ’¾ Cache Performance:
   â€¢ Requests: 100
   â€¢ Hits: 23
   â€¢ Hit rate: 23.0%
   â€¢ Est. tokens saved: 3,611
==================================================
ğŸ’¾ Evaluation results saved to database
```

## ğŸ“ Ejemplo PrÃ¡ctico Completo

```typescript
import { EvalRunner } from './eval-runner';

async function ejemploCompleto() {
  // 1. ğŸ—ï¸ Crear el ejecutor
  const runner = new EvalRunner('./mi-registry.yaml', {
    enabled: true,        // Habilitar cachÃ©
    provider: 'redis',    // Usar Redis para cachÃ© compartido
    ttl_seconds: 3600     // CachÃ© vÃ¡lido por 1 hora
  });

  // 2. ğŸš€ Ejecutar evaluaciÃ³n
  const report = await runner.runEval({
    model: 'gpt-4',                    // Modelo a evaluar
    eval: 'matematicas-basicas',       // EvaluaciÃ³n a usar
    max_samples: 50,                   // Solo 50 preguntas para probar
    temperature: 0.0,                  // Respuestas determinÃ­sticas  
    max_tokens: 10,                    // Respuestas cortas
    timeout: 30000,                    // 30 segundos de timeout
    verbose: true,                     // Mostrar progreso detallado
    log_to_file: 'mi-evaluacion.jsonl', // Guardar logs
    custom_metrics: ['cost-efficiency', 'token-efficiency'], // MÃ©tricas especÃ­ficas
    dry_run: false                     // Ejecutar de verdad
  });

  // 3. ğŸ“Š Usar los resultados
  console.log(`PuntuaciÃ³n final: ${(report.score * 100).toFixed(2)}%`);
  console.log(`Costo total: $${report.token_usage?.estimated_cost.toFixed(4)}`);
  
  // 4. ğŸ” Analizar resultados especÃ­ficos
  const fallos = report.results.filter(r => !r.passed);
  console.log(`Preguntas que fallaron: ${fallos.length}`);
  
  for (const fallo of fallos.slice(0, 3)) { // Primeros 3 fallos
    console.log(`âŒ Pregunta: ${fallo.input[0].content}`);
    console.log(`   Esperado: ${fallo.ideal}`);
    console.log(`   Obtuvo: ${fallo.completion.content}`);
    console.log(`   RazÃ³n: ${fallo.reasoning}`);
  }
}
```

## ğŸ¯ Casos de Uso Reales

### ğŸ§ª EvaluaciÃ³n de Desarrollo
```typescript
// Para desarrolladores: prueba rÃ¡pida y barata
const reportDev = await runner.runEval({
  model: 'gpt-3.5-turbo',  // Modelo mÃ¡s barato
  eval: 'mi-evaluacion',
  max_samples: 10,         // Solo 10 muestras
  dry_run: true           // SimulaciÃ³n primero
});
```

### ğŸš€ EvaluaciÃ³n de ProducciÃ³n  
```typescript
// Para producciÃ³n: evaluaciÃ³n completa
const reportProd = await runner.runEval({
  model: 'gpt-4',
  eval: 'evaluacion-completa',
  // max_samples: undefined,  // Todas las muestras
  temperature: 0.0,        // DeterminÃ­stico
  verbose: false,          // Output limpio
  custom_metrics: ['all'], // Todas las mÃ©tricas
  log_to_file: `prod-eval-${new Date().toISOString()}.jsonl`
});
```

### ğŸ“Š ComparaciÃ³n de Modelos
```typescript
// Evaluar mÃºltiples modelos con el mismo dataset
const modelos = ['gpt-3.5-turbo', 'gpt-4', 'claude-3-sonnet'];
const reportes = [];

for (const modelo of modelos) {
  const reporte = await runner.runEval({
    model: modelo,
    eval: 'comparacion-modelos',
    max_samples: 100,
    temperature: 0.0
  });
  reportes.push(reporte);
}

// Comparar resultados
const comparison = reportes.map(r => ({
  modelo: r.model,
  puntuacion: r.score,
  costo: r.token_usage?.estimated_cost || 0,
  eficiencia: r.score / (r.token_usage?.estimated_cost || 1)
}));

console.table(comparison);
```

## ğŸš¨ Puntos Importantes a Recordar

### âœ… Mejores PrÃ¡cticas
1. **ğŸ§ª Siempre usar dry_run primero** â†’ Evita sorpresas costosas
2. **ğŸ’¾ Habilitar cachÃ© para desarrollo** â†’ Ahorra tiempo y dinero
3. **ğŸ“Š Usar verbose en desarrollo** â†’ Entiende quÃ© estÃ¡ pasando
4. **ğŸ¯ Limitar samples al desarrollar** â†’ IteraciÃ³n mÃ¡s rÃ¡pida
5. **ğŸ“ Siempre especificar log_to_file** â†’ Para debugging y anÃ¡lisis

### âš ï¸ Cuidados Importantes
1. **ğŸ’° Los costos se acumulan rÃ¡pido** â†’ Monitorea el token usage
2. **â±ï¸ Los timeouts deben ser realistas** â†’ Especialmente para modelos locales
3. **ğŸ”„ El cachÃ© puede usar mucha memoria** â†’ Configura lÃ­mites apropiados
4. **ğŸ“Š Las mÃ©tricas personalizadas toman tiempo** â†’ Pueden agregar latencia

### ğŸ› Problemas Comunes
- **"Registry not found"** â†’ Verifica la ruta al archivo de configuraciÃ³n
- **"Model not supported"** â†’ Verifica que el modelo estÃ© disponible
- **"Cache connection failed"** â†’ Verifica que Redis estÃ© corriendo
- **"API timeout"** â†’ Aumenta el timeout o usa un modelo mÃ¡s rÃ¡pido

**Â¡Siguiente paso:** Vamos a ver cÃ³mo funciona el Registry, el catÃ¡logo de configuraciones! ğŸ“‹
