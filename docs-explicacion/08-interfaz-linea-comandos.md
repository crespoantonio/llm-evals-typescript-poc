# ðŸ–¥ï¸ Interfaz de LÃ­nea de Comandos - Tu Varita MÃ¡gica

## Â¿QuÃ© es la CLI?

La **CLI** (Command Line Interface) es como una **varita mÃ¡gica** que te permite controlar todo el framework con comandos simples. Imagina poder decir hechizos como:

- `llm-eval gpt-4 math-basic` â†’ "Â¡EvalÃºa GPT-4 con matemÃ¡ticas bÃ¡sicas!"
- `llm-eval gemini-2.0-flash-001 math-basic` â†’ "Â¡EvalÃºa Google Gemini con matemÃ¡ticas!"
- `llm-eval ollama/llama2 math-basic` â†’ "Â¡EvalÃºa Llama local con matemÃ¡ticas!"
- `llm-eval list` â†’ "Â¡MuÃ©strame todas las evaluaciones disponibles!"
- `llm-eval dashboard` â†’ "Â¡Abre el dashboard mÃ¡gico!"

**Â¿Por quÃ© usar CLI en lugar de cÃ³digo?**
- **ðŸš€ MÃ¡s rÃ¡pido** â†’ Un comando vs escribir cÃ³digo completo
- **ðŸ”§ MÃ¡s flexible** â†’ Cambiar parÃ¡metros sin tocar cÃ³digo
- **ðŸ“Š AnÃ¡lisis instantÃ¡neo** â†’ Comandos para estadÃ­sticas y reportes
- **ðŸ¤– Automatizable** â†’ Perfecta para scripts y CI/CD

## ðŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n del CLI

### ðŸ”— OpciÃ³n 1: Comando Global con `npm link` (Recomendado)

**La mejor opciÃ³n para desarrollo** - NO instala dependencias globalmente:

```bash
# 1. Compilar el proyecto
npm run build

# 2. Crear enlace simbÃ³lico (symlink) - Â¡MUY superior a npm install -g!
npm link

# 3. Â¡Ya puedes usar llm-eval desde cualquier lugar!
llm-eval --help
llm-eval gpt-4 math-basic
llm-eval list

# Para desinstalar limpiamente
npm unlink -g llm-evals-ts
```

**Â¿Por quÃ© `npm link` es mejor que `npm install -g`?**
- âœ… **NO instala dependencias globalmente** 
- âœ… Crea un **enlace simbÃ³lico** al proyecto local
- âœ… Los **cambios se reflejan inmediatamente** sin reinstalar
- âœ… Usa las dependencias ya instaladas en `node_modules/`
- âœ… **PrÃ¡ctica estÃ¡ndar** para desarrollo de CLIs

### âš¡ OpciÃ³n 2: Script de Desarrollo (Para cambios frecuentes)

Si estÃ¡s desarrollando o modificando el framework:

```bash
# Usar el script npm dev
npm run dev -- gpt-4 math-basic
npm run dev -- --help
npm run dev -- list
```

### ðŸ”§ OpciÃ³n 3: Uso Directo con ts-node

Para uso ocasional sin instalaciÃ³n global:

```bash
# Ejecutar directamente desde el cÃ³digo fuente
npx ts-node src/cli.ts gpt-4 math-basic
npx ts-node src/cli.ts --help
npx ts-node src/cli.ts list
```

### ðŸ“¦ OpciÃ³n 4: InstalaciÃ³n Global Completa (No recomendada)

âš ï¸ **Solo si `npm link` no funciona en tu sistema:**

```bash
# Instala todo globalmente (incluyendo dependencias)
npm run build
npm install -g .

# Para desinstalar
npm uninstall -g llm-evals-ts
```

**Desventajas:** Instala todas las dependencias globalmente, ocupa mÃ¡s espacio.

### âš ï¸ Troubleshooting

Si `llm-eval` no funciona despuÃ©s de usar `npm link`:

```bash
# 1. Desenlazar y volver a enlazar
npm unlink -g llm-evals-ts
npm run build
npm link

# 2. Verificar instalaciÃ³n
llm-eval --version
which llm-eval  # Debe apuntar a tu directorio del proyecto

# 3. Si sigue fallando, usar npm run dev como alternativa
npm run dev -- --help

# 4. Verificar que package.json tiene el campo "bin" correcto
cat package.json | grep -A2 '"bin"'
```

### ðŸŽ¯ Verificar que Todo Funciona

```bash
# 1. Inicializar registry (si no existe)
llm-eval init

# 2. Listar evaluaciones disponibles
llm-eval list

# 3. Ejecutar prueba sin gastar dinero
llm-eval gpt-3.5-turbo math-basic --dry-run --max-samples 3
```

## ðŸ§™â€â™‚ï¸ Los Hechizos Principales

### 1. âš¡ Comando Principal - Ejecutar Evaluaciones

```bash
# Hechizo bÃ¡sico
llm-eval gpt-4 math-basic

# Hechizo con parÃ¡metros mÃ¡gicos
llm-eval gpt-4 math-basic \
  --max-samples 50 \
  --temperature 0.0 \
  --verbose \
  --log-to-file mi-evaluacion.jsonl
```

**Â¿QuÃ© hace cada parÃ¡metro?**

- **`gpt-4`** â†’ El modelo de IA que quieres evaluar
- **`math-basic`** â†’ La evaluaciÃ³n que quieres ejecutar  
- **`--max-samples 50`** â†’ Solo evaluar 50 preguntas (en lugar de todas)
- **`--temperature 0.0`** â†’ Respuestas determinÃ­sticas (0.0 = mÃ¡s predecible, 2.0 = mÃ¡s creativo)
- **`--verbose`** â†’ Mostrar progreso detallado
- **`--log-to-file`** â†’ Guardar logs detallados en un archivo

### 2. ðŸ“‹ Hechizo para Ver Evaluaciones

```bash
llm-eval list
```

**Resultado:**
```
ðŸ“‹ Available Evaluations
Registry: ./registry

math-basic
  Basic math evaluation using exact matching
  Metrics: accuracy
  Class: BasicEval

history-essays
  Historical essay evaluation using semantic similarity
  Metrics: accuracy, semantic_similarity
  Class: SemanticSimilarityEval
```

### 3. ðŸ—ï¸ Hechizo de InicializaciÃ³n

```bash
llm-eval init [ruta-opcional]
```

**Â¿QuÃ© hace?**
- âœ… Crea la estructura de carpetas del registry
- âœ… Genera configuraciones de ejemplo
- âœ… Crea datasets de muestra
- âœ… Te da instrucciones de quÃ© hacer despuÃ©s

## ðŸŽ­ Modos Especiales de EjecuciÃ³n

### ðŸ§ª Modo Dry Run (SimulaciÃ³n)

```bash
llm-eval gpt-4 math-basic --dry-run
```

**Es como:** Ensayar una obra de teatro sin subir al escenario real

**Â¿QuÃ© hace?**
- âœ… No hace peticiones reales a la API (no gastas dinero)
- âœ… Te muestra quÃ© preguntas ejecutarÃ­a
- âœ… Te permite verificar configuraciones
- âœ… Estima costos sin gastar

**CuÃ¡ndo usar:**
- ðŸ” Verificar que el dataset estÃ¡ correcto
- ðŸ’° Estimar costos antes de ejecutar
- âš™ï¸ Probar configuraciones nuevas

### ðŸ” Modo Verbose (Detallado)

```bash
llm-eval gpt-4 math-basic --verbose
```

**Resultado:**
```
ðŸ§  LLM Evaluation Framework
Model: gpt-4 | Eval: math-basic

Loading dataset from: registry/data/math/basic.jsonl
Evaluating 100 samples

Progress: 1/100 (1%)
âœ… Sample 1 - Score: 1.0 - PASS
   Reasoning: La respuesta '5' coincide exactamente con el ideal '5'
ðŸ’¾ Cache hit for sample 2
âœ… Sample 2 - Score: 1.0 - PASS

ðŸ“Š Calculated 3 custom metrics
ðŸ’¾ Cache Performance:
   â€¢ Requests: 100
   â€¢ Hits: 23
   â€¢ Hit rate: 23.0%
```

### ðŸŽ¯ LimitaciÃ³n de Muestras

```bash
# Solo las primeras 10 preguntas (perfecto para testing)
llm-eval gpt-4 math-basic --max-samples 10

# Ideal para desarrollo rÃ¡pido
llm-eval gpt-3.5-turbo math-basic --max-samples 5 --dry-run
```

## ðŸ“Š Comandos de AnÃ¡lisis - Los Hechizos de EstadÃ­sticas

### ðŸ’° AnÃ¡lisis de Costos

```bash
# Desglose de costos de los Ãºltimos 30 dÃ­as
llm-eval costs breakdown

# Desglose de la Ãºltima semana
llm-eval costs breakdown 7

# Predecir costos para una evaluaciÃ³n futura
llm-eval costs predict gpt-4 math-basic 100

# EstimaciÃ³n rÃ¡pida
llm-eval costs estimate gpt-4 50 --input-length 300 --output-length 100
```

**Resultado de `costs breakdown`:**
```
ðŸ’° Cost Breakdown
Period: Last 30 days

ðŸ“Š Summary:
   Total cost: $12.4567
   Total runs: 45
   Average cost per run: $0.2768

ðŸ’¸ Cost by Evaluation:
   1. math-advanced (34.2%)
      â€¢ Total: $4.26 over 15 runs
      â€¢ Avg per run: $0.284
      â€¢ Models: gpt-4, gpt-3.5-turbo
      â€¢ Trend: increasing ðŸ“ˆ

   2. history-essays (28.7%)
      â€¢ Total: $3.58 over 12 runs  
      â€¢ Models: gpt-4
      â€¢ Trend: stable âž¡ï¸
```

### ðŸ“ˆ AnÃ¡lisis de Tokens

```bash
# Reporte general de tokens
llm-eval tokens report

# Tendencias de una evaluaciÃ³n especÃ­fica
llm-eval tokens trends math-basic 14  # Ãšltimos 14 dÃ­as

# Comparar eficiencia entre modelos
llm-eval tokens efficiency gpt-4 gpt-3.5-turbo claude-3-sonnet
```

**Resultado de `tokens report`:**
```
ðŸ“Š Token Analytics Report
Period: Last 30 days

ðŸ“ˆ Summary:
   Total evaluations: 87
   Total tokens: 1,247,839
   Total cost: $18.7234
   Most efficient model: gpt-3.5-turbo
   Most expensive model: gpt-4

ðŸ† Model Efficiency Ranking:
   1. gpt-3.5-turbo - $0.0034/sample (127 tokens)
   2. claude-3-sonnet - $0.0045/sample (134 tokens)
   3. gpt-4 - $0.0089/sample (145 tokens)

ðŸ’¡ Recommendations:
   â€¢ Consider using gpt-3.5-turbo for cost-sensitive evaluations
   â€¢ gpt-4 shows 23% higher accuracy but 160% higher cost
```

### ðŸŽ¯ AnÃ¡lisis de MÃ©tricas

```bash
# Listar mÃ©tricas disponibles
llm-eval metrics list

# Probar mÃ©tricas en una evaluaciÃ³n existente
llm-eval metrics test math-basic
```

## ðŸ’¾ Comandos de GestiÃ³n de CachÃ©

### âš¡ Cache - Tu Ahorro Inteligente

```bash
# Ver estadÃ­sticas de cachÃ©
llm-eval cache stats

# Limpiar todo el cachÃ© (Â¡cuidado!)
llm-eval cache clear

# Invalidar cachÃ© de un modelo especÃ­fico
llm-eval cache invalidate gpt-4
```

**Resultado de `cache stats`:**
```
ðŸ’¾ Cache Statistics

ðŸ“Š Performance:
   â€¢ Total requests: 1,247
   â€¢ Cache hits: 312
   â€¢ Cache misses: 935
   â€¢ Hit rate: 25.0%
   â€¢ Memory usage: 45.2 MB

ðŸ’° Savings:
   â€¢ Estimated tokens saved: 78,450
   â€¢ Estimated cost saved: $1.23
   â€¢ Average response time: 45ms (cached) vs 2,341ms (API)
```

## ðŸ“± Dashboard - Tu Centro de Control

```bash
# Iniciar dashboard en puerto por defecto (3000)
llm-eval dashboard

# Usar puerto personalizado
llm-eval dashboard 8080
```

**Â¿QuÃ© obtienes?**
- ðŸ“Š **GrÃ¡ficas interactivas** de rendimiento
- ðŸ’° **AnÃ¡lisis de costos en tiempo real**
- ðŸ“ˆ **Tendencias de accuracy**
- ðŸ” **ExploraciÃ³n detallada de resultados**
- ðŸ“‹ **Comparaciones entre modelos**

## ðŸ› ï¸ Opciones Globales - Los Modificadores Universales

### Opciones que funcionan con todos los comandos:

```bash
# Registry personalizado
llm-eval gpt-4 math-basic --registry ./mi-registry

# Diferentes modelos con sus timeouts recomendados
llm-eval gemini-2.0-flash-001 math-basic --timeout 120000  # Google: 2 minutos
llm-eval ollama/llama3.1 math-basic --timeout 300000      # Ollama: 5 minutos
llm-eval hf/google/flan-t5-large math-basic --timeout 180000  # HuggingFace: 3 minutos

# Temperatura personalizada por modelo
llm-eval gpt-4 creative-writing --temperature 1.2          # OpenAI mÃ¡s creativo
llm-eval google/gemini-1.5-pro creative-writing --temperature 0.9  # Google creativo

# LÃ­mite de tokens
llm-eval gemini-2.0-flash-001 short-answers --max-tokens 50

# Semilla para reproducibilidad
llm-eval gpt-4 math-basic --seed 42  # Siempre los mismos resultados

# Logs personalizados
llm-eval google/gemini-1.5-pro math-basic --log-to-file experimento-gemini.jsonl
```

## ðŸ’¡ Ejemplos de Workflows Reales

### ðŸ§ª Workflow de Desarrollo

```bash
# 1. Verificar que todo estÃ¡ bien (sin gastar dinero)
llm-eval gpt-3.5-turbo math-basic --dry-run --max-samples 5

# 2. Prueba rÃ¡pida con pocas muestras
llm-eval gpt-3.5-turbo math-basic --max-samples 10 --verbose

# 3. Si funciona bien, evaluaciÃ³n completa
llm-eval gpt-3.5-turbo math-basic --log-to-file dev-test.jsonl
```

### ðŸš€ Workflow de ProducciÃ³n

```bash
# 1. Estimar costos primero
llm-eval costs predict gpt-4 production-eval 500

# 2. Ejecutar evaluaciÃ³n completa
llm-eval gpt-4 production-eval \
  --temperature 0.0 \
  --log-to-file prod-$(date +%Y%m%d).jsonl \
  --verbose

# 3. AnÃ¡lisis posterior
llm-eval tokens report 1  # Ãšltimo dÃ­a
llm-eval costs breakdown 1
```

### ðŸ“Š Workflow de ComparaciÃ³n

```bash
# 1. Evaluar mÃºltiples modelos
llm-eval gpt-4 comparison-test --log-to-file gpt4-results.jsonl
llm-eval gpt-3.5-turbo comparison-test --log-to-file gpt35-results.jsonl
llm-eval claude-3-sonnet comparison-test --log-to-file claude-results.jsonl

# 2. Comparar eficiencia
llm-eval tokens efficiency gpt-4 gpt-3.5-turbo claude-3-sonnet --eval comparison-test

# 3. Iniciar dashboard para anÃ¡lisis visual
llm-eval dashboard
```

### ðŸ”„ Workflow de ExperimentaciÃ³n

```bash
# Experimento con diferentes temperaturas
for temp in 0.0 0.3 0.7 1.0; do
  llm-eval gpt-4 creative-eval \
    --temperature $temp \
    --max-samples 20 \
    --log-to-file "experiment-temp-$temp.jsonl"
done

# Experimento con diferentes lÃ­mites de tokens
for tokens in 50 100 200 500; do
  llm-eval gpt-4 summary-eval \
    --max-tokens $tokens \
    --max-samples 20 \
    --log-to-file "experiment-tokens-$tokens.jsonl"
done
```

## ðŸš¨ ValidaciÃ³n AutomÃ¡tica de Entorno

La CLI es inteligente y verifica tu configuraciÃ³n automÃ¡ticamente:

```bash
llm-eval gpt-4 math-basic
```

**Si tu .env no estÃ¡ configurado correctamente:**
```
ðŸ” Environment Validation:
âŒ OPENAI_API_KEY: Missing or invalid
   Expected format: sk-...
   Set up: Create .env file with OPENAI_API_KEY=your_key

âš ï¸  Redis: Not configured (optional)
   Cache will use memory instead
   For better performance, install Redis

âœ… Registry: Found at ./registry

Cannot proceed without valid OPENAI_API_KEY
```

## ðŸŽ¯ Tips y Trucos Profesionales

### ðŸ’° Control de Costos
```bash
# Siempre estimar antes de ejecutar
llm-eval costs estimate gpt-4 100 && \
llm-eval gpt-4 math-basic --max-samples 100

# Usar modelos mÃ¡s baratos para desarrollo
llm-eval gpt-3.5-turbo math-basic --max-samples 10  # Desarrollo
llm-eval gpt-4 math-basic  # ProducciÃ³n
```

### âš¡ OptimizaciÃ³n de Velocidad
```bash
# Usar cachÃ© Redis para equipos
export REDIS_URL=redis://localhost:6379
llm-eval gpt-4 math-basic  # CompartirÃ¡ cachÃ© con el equipo

# Timeouts apropiados por tipo de modelo
llm-eval gpt-4 math-basic --timeout 60000                 # OpenAI: 1 minuto
llm-eval gemini-2.0-flash-001 math-basic --timeout 120000 # Google: 2 minutos
llm-eval hf/google/flan-t5-large math-basic --timeout 180000  # HuggingFace: 3 minutos
llm-eval ollama/llama3.1 math-basic --timeout 300000      # Ollama Local: 5 minutos
```

### ðŸ“Š AnÃ¡lisis SistemÃ¡tico
```bash
# Crear reports periÃ³dicos
llm-eval tokens report 7 > weekly-report.txt
llm-eval costs breakdown 7 >> weekly-report.txt

# Monitoreo de tendencias
llm-eval tokens trends production-eval 30
```

## ðŸŽ“ Puntos Clave para Recordar

1. **Cuatro proveedores soportados** â†’ OpenAI, Google Gen AI, Ollama, HuggingFace
2. **La CLI es tu navaja suiza** â†’ Un comando para cada necesidad
3. **Dry run es tu mejor amigo** â†’ Nunca gastes dinero sin probar
4. **Timeouts por proveedor** â†’ OpenAI (1min), Google (2min), HF (3min), Ollama (5min)
5. **El verbose mode es para debugging** â†’ Ãšsalo cuando algo falle
6. **Los logs son oro** â†’ Siempre especifica --log-to-file
7. **El dashboard visualiza todo** â†’ Para anÃ¡lisis complejos
8. **La validaciÃ³n automÃ¡tica previene errores** â†’ ConfÃ­a en los mensajes
9. **Los comandos de anÃ¡lisis ahorran tiempo** â†’ No reinventes la rueda

### ðŸš€ Comando de Ayuda Universal

Cuando tengas dudas:
```bash
llm-eval --help              # Ayuda general
llm-eval costs --help        # Ayuda de costos
llm-eval tokens --help       # Ayuda de tokens
llm-eval dashboard --help    # Ayuda del dashboard
```

**Â¡Siguiente paso:** Vamos a explorar los diferentes tipos de evaluaciones (templates)! ðŸ§ª
